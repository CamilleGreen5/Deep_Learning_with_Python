{
  "nbformat": 4,
  "nbformat_minor": 0,
  "metadata": {
    "colab": {
      "name": "Noise_speech_reduction.ipynb",
      "provenance": [],
      "collapsed_sections": [
        "q5NDwppQei-Q",
        "5rrtLuCRfpHW",
        "P9KkuU5XO6yR"
      ],
      "toc_visible": true,
      "authorship_tag": "ABX9TyOc6N3A7MX6bGVpcUI84RVm",
      "include_colab_link": true
    },
    "kernelspec": {
      "name": "python3",
      "display_name": "Python 3"
    },
    "widgets": {
      "application/vnd.jupyter.widget-state+json": {
        "94e6fdeca98a4d0b8c7ce52e6162ecd0": {
          "model_module": "@jupyter-widgets/controls",
          "model_name": "HBoxModel",
          "state": {
            "_view_name": "HBoxView",
            "_dom_classes": [],
            "_model_name": "HBoxModel",
            "_view_module": "@jupyter-widgets/controls",
            "_model_module_version": "1.5.0",
            "_view_count": null,
            "_view_module_version": "1.5.0",
            "box_style": "",
            "layout": "IPY_MODEL_774c0af238a24314b0257646698fe490",
            "_model_module": "@jupyter-widgets/controls",
            "children": [
              "IPY_MODEL_96a124720fdb42a3a7e025a3329d440a",
              "IPY_MODEL_4df34abfa3eb41c5981bbd460670739a"
            ]
          }
        },
        "774c0af238a24314b0257646698fe490": {
          "model_module": "@jupyter-widgets/base",
          "model_name": "LayoutModel",
          "state": {
            "_view_name": "LayoutView",
            "grid_template_rows": null,
            "right": null,
            "justify_content": null,
            "_view_module": "@jupyter-widgets/base",
            "overflow": null,
            "_model_module_version": "1.2.0",
            "_view_count": null,
            "flex_flow": null,
            "width": null,
            "min_width": null,
            "border": null,
            "align_items": null,
            "bottom": null,
            "_model_module": "@jupyter-widgets/base",
            "top": null,
            "grid_column": null,
            "overflow_y": null,
            "overflow_x": null,
            "grid_auto_flow": null,
            "grid_area": null,
            "grid_template_columns": null,
            "flex": null,
            "_model_name": "LayoutModel",
            "justify_items": null,
            "grid_row": null,
            "max_height": null,
            "align_content": null,
            "visibility": null,
            "align_self": null,
            "height": null,
            "min_height": null,
            "padding": null,
            "grid_auto_rows": null,
            "grid_gap": null,
            "max_width": null,
            "order": null,
            "_view_module_version": "1.2.0",
            "grid_template_areas": null,
            "object_position": null,
            "object_fit": null,
            "grid_auto_columns": null,
            "margin": null,
            "display": null,
            "left": null
          }
        },
        "96a124720fdb42a3a7e025a3329d440a": {
          "model_module": "@jupyter-widgets/controls",
          "model_name": "FloatProgressModel",
          "state": {
            "_view_name": "ProgressView",
            "style": "IPY_MODEL_1918b54b8e624da485e46afc32cff7f2",
            "_dom_classes": [],
            "description": "100%",
            "_model_name": "FloatProgressModel",
            "bar_style": "success",
            "max": 1,
            "_view_module": "@jupyter-widgets/controls",
            "_model_module_version": "1.5.0",
            "value": 1,
            "_view_count": null,
            "_view_module_version": "1.5.0",
            "orientation": "horizontal",
            "min": 0,
            "description_tooltip": null,
            "_model_module": "@jupyter-widgets/controls",
            "layout": "IPY_MODEL_37ae2d8f98424c71951abecbdb32617e"
          }
        },
        "4df34abfa3eb41c5981bbd460670739a": {
          "model_module": "@jupyter-widgets/controls",
          "model_name": "HTMLModel",
          "state": {
            "_view_name": "HTMLView",
            "style": "IPY_MODEL_627ed32a482a4a5dac594dcb0e61186e",
            "_dom_classes": [],
            "description": "",
            "_model_name": "HTMLModel",
            "placeholder": "​",
            "_view_module": "@jupyter-widgets/controls",
            "_model_module_version": "1.5.0",
            "value": " 1/1 [03:10&lt;00:00, 190.56s/it]",
            "_view_count": null,
            "_view_module_version": "1.5.0",
            "description_tooltip": null,
            "_model_module": "@jupyter-widgets/controls",
            "layout": "IPY_MODEL_cb367e16a13a4bda87e91a76d8100782"
          }
        },
        "1918b54b8e624da485e46afc32cff7f2": {
          "model_module": "@jupyter-widgets/controls",
          "model_name": "ProgressStyleModel",
          "state": {
            "_view_name": "StyleView",
            "_model_name": "ProgressStyleModel",
            "description_width": "initial",
            "_view_module": "@jupyter-widgets/base",
            "_model_module_version": "1.5.0",
            "_view_count": null,
            "_view_module_version": "1.2.0",
            "bar_color": null,
            "_model_module": "@jupyter-widgets/controls"
          }
        },
        "37ae2d8f98424c71951abecbdb32617e": {
          "model_module": "@jupyter-widgets/base",
          "model_name": "LayoutModel",
          "state": {
            "_view_name": "LayoutView",
            "grid_template_rows": null,
            "right": null,
            "justify_content": null,
            "_view_module": "@jupyter-widgets/base",
            "overflow": null,
            "_model_module_version": "1.2.0",
            "_view_count": null,
            "flex_flow": null,
            "width": null,
            "min_width": null,
            "border": null,
            "align_items": null,
            "bottom": null,
            "_model_module": "@jupyter-widgets/base",
            "top": null,
            "grid_column": null,
            "overflow_y": null,
            "overflow_x": null,
            "grid_auto_flow": null,
            "grid_area": null,
            "grid_template_columns": null,
            "flex": null,
            "_model_name": "LayoutModel",
            "justify_items": null,
            "grid_row": null,
            "max_height": null,
            "align_content": null,
            "visibility": null,
            "align_self": null,
            "height": null,
            "min_height": null,
            "padding": null,
            "grid_auto_rows": null,
            "grid_gap": null,
            "max_width": null,
            "order": null,
            "_view_module_version": "1.2.0",
            "grid_template_areas": null,
            "object_position": null,
            "object_fit": null,
            "grid_auto_columns": null,
            "margin": null,
            "display": null,
            "left": null
          }
        },
        "627ed32a482a4a5dac594dcb0e61186e": {
          "model_module": "@jupyter-widgets/controls",
          "model_name": "DescriptionStyleModel",
          "state": {
            "_view_name": "StyleView",
            "_model_name": "DescriptionStyleModel",
            "description_width": "",
            "_view_module": "@jupyter-widgets/base",
            "_model_module_version": "1.5.0",
            "_view_count": null,
            "_view_module_version": "1.2.0",
            "_model_module": "@jupyter-widgets/controls"
          }
        },
        "cb367e16a13a4bda87e91a76d8100782": {
          "model_module": "@jupyter-widgets/base",
          "model_name": "LayoutModel",
          "state": {
            "_view_name": "LayoutView",
            "grid_template_rows": null,
            "right": null,
            "justify_content": null,
            "_view_module": "@jupyter-widgets/base",
            "overflow": null,
            "_model_module_version": "1.2.0",
            "_view_count": null,
            "flex_flow": null,
            "width": null,
            "min_width": null,
            "border": null,
            "align_items": null,
            "bottom": null,
            "_model_module": "@jupyter-widgets/base",
            "top": null,
            "grid_column": null,
            "overflow_y": null,
            "overflow_x": null,
            "grid_auto_flow": null,
            "grid_area": null,
            "grid_template_columns": null,
            "flex": null,
            "_model_name": "LayoutModel",
            "justify_items": null,
            "grid_row": null,
            "max_height": null,
            "align_content": null,
            "visibility": null,
            "align_self": null,
            "height": null,
            "min_height": null,
            "padding": null,
            "grid_auto_rows": null,
            "grid_gap": null,
            "max_width": null,
            "order": null,
            "_view_module_version": "1.2.0",
            "grid_template_areas": null,
            "object_position": null,
            "object_fit": null,
            "grid_auto_columns": null,
            "margin": null,
            "display": null,
            "left": null
          }
        }
      }
    }
  },
  "cells": [
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "view-in-github",
        "colab_type": "text"
      },
      "source": [
        "<a href=\"https://colab.research.google.com/github/CamilleGreen5/Deep_Learning_with_Python/blob/master/Noise_speech_reduction.ipynb\" target=\"_parent\"><img src=\"https://colab.research.google.com/assets/colab-badge.svg\" alt=\"Open In Colab\"/></a>"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "njrINNDksie4"
      },
      "source": [
        "INFO :\n",
        "- methode des masks donnent mauvais résultats (avec label)"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "q5NDwppQei-Q"
      },
      "source": [
        "## **DATA LOADING**"
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "tfXnXSBj-05D"
      },
      "source": [
        "# !pip install pydub"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "RzPdSF9gRJcb"
      },
      "source": [
        "# !wget https://voice-prod-bundler-ee1969a6ce8178826482b88e843c335139bd3fb4.s3.amazonaws.com/cv-corpus-5.1-2020-06-22/fr.tar.gz\n",
        "# !tar -xf fr.tar.gz"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "ITu5Puvoakzl"
      },
      "source": [
        "# creating folders\n",
        "\n",
        "!rm fr.tar.gz\n",
        "!rm -r sample_data\n",
        "\n",
        "!mkdir sons\n",
        "!mkdir sons/original\n",
        "!mkdir sons/noisy\n",
        "!mkdir sons/denoised\n",
        "\n",
        "!mkdir data\n",
        "!mkdir data/train_data\n",
        "!mkdir data/val_data\n",
        "!mkdir data/test_data\n",
        "!mkdir data/train_data/folder1\n",
        "!mkdir data/val_data/folder1\n",
        "!mkdir data/test_data/folder1\n",
        "\n",
        "!cp cv-corpus-5.1-2020-06-22/fr/clips/common_voice_fr_18157595.mp3 ./sons/original"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "Y9S0MO3jee8d"
      },
      "source": [
        "## **FUNCTIONS AND LIBRARIES LOAD**"
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "buYbi1H55f_3"
      },
      "source": [
        "# Load the TensorBoard notebook extension\n",
        "%reload_ext tensorboard"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "G7GhHp8oNRF9"
      },
      "source": [
        "import tensorflow as tf\n",
        "import tensorflow_datasets as tfds\n",
        "import numpy as np\n",
        "import matplotlib.pyplot as plt\n",
        "import glob\n",
        "import datetime\n",
        "import os\n",
        "import cv2 as cv\n",
        "from scipy.io.wavfile import read, write\n",
        "from scipy.signal import stft, istft\n",
        "from pydub import AudioSegment\n",
        "from tensorflow.keras import models, layers\n",
        "from tqdm.notebook import tqdm"
      ],
      "execution_count": 4,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "hydHLwi8eyw5"
      },
      "source": [
        "## **DATA CONVERSION**"
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "aOdsq0ea13dj"
      },
      "source": [
        "def add_noise(array_sound):\n",
        "    \"\"\"\n",
        "    function to add gaussian noise to an array\n",
        "    -- array_sound : array which to add noise\n",
        "    -- return : the noisy array\n",
        "    \"\"\"\n",
        "    max_1 = np.max(array_sound)\n",
        "    noise = np.random.normal(0, 0.02*max_1, len(array_sound))\n",
        "    array_sound_noisy = np.add(noise, array_sound)\n",
        "    max_2 = np.max(array_sound_noisy)\n",
        "    array_sound_noisy = (array_sound_noisy*max_1)//max_2\n",
        "\n",
        "    return np.array(array_sound_noisy, np.int16)"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "B2TT4gVTf4sK"
      },
      "source": [
        "def load_and_convert_data_with_label(train_mp3_data, mode='train'):\n",
        "    \"\"\"\n",
        "    function to load data, convert them to array and save as image.\n",
        "    sample and label, are concanated to be saved as one image\n",
        "    -- train_mp3_data : list of mp3 data to convert\n",
        "    -- mode : train/val/test to indicate where to save images\n",
        "    \"\"\"\n",
        "    i = 0\n",
        "\n",
        "    for path_to_mp3_file in tqdm(train_mp3_data):\n",
        "\n",
        "        # load mp3 sound to array\n",
        "        loaded_sound = AudioSegment.from_mp3(path_to_mp3_file)\n",
        "        array_sound = np.array(loaded_sound.get_array_of_samples(), np.int16)\n",
        "        # print(np.max(np.max(array_sound)))\n",
        "        \n",
        "        # normalize array\n",
        "        mean = np.mean(array_sound, axis=0)\n",
        "        array_sound -= int(mean)\n",
        "\n",
        "        # convert label to to F_T_space\n",
        "        f, t, array_stft = stft(array_sound, fs=48000, nperseg=1000)\n",
        "        original_abs_stft = np.abs(array_stft[:,:1000])\n",
        "\n",
        "        # padding\n",
        "        pad_length = 1000 - np.shape(original_abs_stft)[1]\n",
        "        # print(pad_length)\n",
        "        original_abs_stft = np.pad(original_abs_stft, ((0,0), (0, pad_length)))\n",
        "        # print(np.shape(original_abs_stft))\n",
        "        \n",
        "        # add random noise\n",
        "        array_sound_noisy = add_noise(array_sound)\n",
        "\n",
        "        # plot original and noisy\n",
        "        # plt.plot([i for i in range(len(array_sound_noisy))], array_sound_noisy, 'r')\n",
        "        # plt.plot([i for i in range(len(array_sound))], array_sound, 'b')\n",
        "        # plt.show()\n",
        "\n",
        "        # save noisy sound to wav\n",
        "        # path_to_wav_file = './sons/noisy/noisy_sound0.wav'\n",
        "        # write(path_to_wav_file, 48000, array_sound_noisy)\n",
        "\n",
        "        # convert sample to to F_T_space\n",
        "        f, t, array_stft = stft(array_sound_noisy, fs=48000, nperseg=1000)\n",
        "        sample_abs_stft = np.abs(array_stft[:,:1000])\n",
        "\n",
        "        # padding \n",
        "        pad_length = 1000 - np.shape(sample_abs_stft)[1]\n",
        "        sample_abs_stft = np.pad(sample_abs_stft, ((0,0), (0, pad_length)))\n",
        "        # print(np.shape(sample_abs_stft))\n",
        "\n",
        "        # convertion to image format\n",
        "        sample_to_save = sample_abs_stft//(np.max(np.max(sample_abs_stft))/32767)\n",
        "        label_to_save = original_abs_stft//(np.max(np.max(original_abs_stft))/32767)\n",
        "        sample_to_save0 = sample_to_save - sample_to_save%1000\n",
        "        sample_to_save1 = sample_to_save - sample_to_save0 - sample_to_save%10\n",
        "        label_to_save0 = label_to_save - label_to_save%1000\n",
        "        label_to_save1 = label_to_save - label_to_save0 - label_to_save%10\n",
        "        sample_label_to_save = np.stack((sample_to_save0, sample_to_save1, label_to_save0, label_to_save1), axis=2)\n",
        "        sample_label_to_save = np.array(sample_label_to_save, np.int16)\n",
        "        # print(np.shape(sample_label_to_save))\n",
        "\n",
        "        # saving images\n",
        "        if mode == 'train':\n",
        "            path_sample = str(\"./data/train_data/folder1/sample\" + str(i) + \".png\")\n",
        "        elif mode == 'val':\n",
        "            path_sample = str(\"./data/val_data/folder1/sample\" + str(i) + \".png\")\n",
        "        elif mode == 'test':\n",
        "            path_sample = str(\"./data/test_data/folder1/sample\" + str(i) + \".png\")\n",
        "        else:\n",
        "            print(\"chose a mode\")\n",
        "        tf.keras.preprocessing.image.save_img(path_sample, sample_label_to_save, data_format='channels_last')\n",
        "\n",
        "        # free space\n",
        "        path = train_mp3_data[i]\n",
        "        try:\n",
        "            os.remove(path)\n",
        "        except FileNotFoundError:\n",
        "            continue\n",
        "\n",
        "        i+=1\n",
        "    "
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "oBv1yBnhcFAr"
      },
      "source": [
        "# !rm data/test_data/folder1/*\n",
        "# !rm data/train_data/folder1/*\n",
        "# !rm data/val_data/folder1/*"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "WOvohzPDeb9U"
      },
      "source": [
        "list_mp3_files = glob.glob(\"./cv-corpus-5.1-2020-06-22/fr/clips/*.mp3\")\n",
        "print(len(list_mp3_files))\n",
        "\n",
        "train_mp3_data = list_mp3_files[:2]\n",
        "# val_mp3_data = list_mp3_files[1000:1250]\n",
        "# test_mp3_data = list_mp3_files[7000:14000]"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "dCwxQ3Z6j-FI"
      },
      "source": [
        "load_and_convert_data_with_label(train_mp3_data, 'train')\n",
        "# load_and_convert_data_with_label(val_mp3_data, 'val')\n",
        "# load_and_convert_data_with_label(test_mp3_data, 'test')"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "_DSWHdQaFr7U"
      },
      "source": [
        "!ls data/train_data/folder1 | wc -l"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "MsBC7dTbfCcD"
      },
      "source": [
        "## **MODEL FITING**"
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "0f500K0Izlr6"
      },
      "source": [
        "# !rm -r logs/fit/*"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "Y5YPnEf4AS7m"
      },
      "source": [
        "def build_model():\n",
        "    \"\"\"\n",
        "    function to build the model\n",
        "    -- return the compiled model\n",
        "    \"\"\"\n",
        "    model = models.Sequential()\n",
        "    model.add(layers.SeparableConv2D(16, (3, 3), activation='relu', padding='same', input_shape=(1000, 501, 1)))\n",
        "    model.add(layers.SeparableConv2D(32, (3, 3), activation='relu', padding='same'))\n",
        "    model.add(layers.SeparableConv2D(1, (3, 3), activation='relu', padding='same'))\n",
        "\n",
        "    model.compile(optimizer='rmsprop', loss='mse', metrics=['mae'])\n",
        "\n",
        "    return model"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "s7JeMASisCLN"
      },
      "source": [
        "def data_generator(mode='train', batch_size=5):\n",
        "    \"\"\"\n",
        "    generator to load image and convert to array of sample and label on the fly\n",
        "    -- mode : train/val/test indicate where to load data\n",
        "    -- batch_size : size of the batch to return\n",
        "    -- return : batch of sample and label\n",
        "    \"\"\"\n",
        "    img_gen = tf.keras.preprocessing.image.ImageDataGenerator()\n",
        "    if mode == 'train':\n",
        "        samples_and_labels = img_gen.flow_from_directory(\"./data/train_data/\", class_mode=None, target_size=(501, 1000), batch_size=1, color_mode='rgba')\n",
        "    elif mode == 'val':\n",
        "        samples_and_labels = img_gen.flow_from_directory(\"./data/val_data/\", class_mode=None, target_size=(501, 1000), batch_size=1, color_mode='rgba')\n",
        "    elif mode == 'test':\n",
        "        samples_and_labels = img_gen.flow_from_directory(\"./data/test_data/\", class_mode=None, target_size=(501, 1000), batch_size=1, color_mode='rgba')  \n",
        "    else:\n",
        "        print(\"chose a mode\")\n",
        "\n",
        "    batch_sample = []\n",
        "    batch_label = []\n",
        "\n",
        "    for sample_and_label in samples_and_labels:\n",
        "        sample_and_label = np.array(sample_and_label, np.int16)\n",
        "        # print(np.shape(sample_and_label))\n",
        "        sample0 = np.squeeze(sample_and_label[:,:,:,0])\n",
        "        sample1 = np.squeeze(sample_and_label[:,:,:,1])\n",
        "        label0 = np.squeeze(sample_and_label[:,:,:,2])\n",
        "        label1 = np.squeeze(sample_and_label[:,:,:,3])\n",
        "        sample = 1000*sample0 + 10*sample1\n",
        "        label = 1000*label0 + 10*label1\n",
        "        max_arr = 32767\n",
        "        sample = np.array(sample/max_arr, np.float32)\n",
        "        label = np.array(label/max_arr, np.float32)\n",
        "\n",
        "        sample = tf.convert_to_tensor(sample)\n",
        "        label = tf.convert_to_tensor(label)\n",
        "\n",
        "        sample = np.transpose(sample)\n",
        "        label = np.transpose(label)\n",
        "\n",
        "        # print(np.shape(sample))\n",
        "        # print(np.shape(label))\n",
        "\n",
        "        if len(batch_sample) == batch_size:\n",
        "            yield batch_sample, batch_label\n",
        "            batch_sample = []\n",
        "            batch_label = []\n",
        "        else:\n",
        "            batch_sample.append(sample)\n",
        "            batch_label.append(label)\n",
        "        \n",
        "def create_train_generator():\n",
        "    gen = data_generator('train')\n",
        "    return gen\n",
        "        \n",
        "def create_val_generator():\n",
        "    gen = data_generator('val')\n",
        "    return gen"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "VZhcL-fY66VE"
      },
      "source": [
        "# Load model, create generator, fit the model and save it to 'model' folder\n",
        "\n",
        "NEW_MODEL = True\n",
        "if NEW_MODEL:\n",
        "    !rm -r logs/fit/*\n",
        "\n",
        "log_dir = \"logs/fit/\" + datetime.datetime.now().strftime(\"%Y%m%d-%H%M%S\")\n",
        "tensorboard_callback = tf.keras.callbacks.TensorBoard(log_dir=log_dir, histogram_freq=1)\n",
        "\n",
        "if NEW_MODEL:\n",
        "    model = build_model()\n",
        "else:\n",
        "    model = tf.keras.models.load_model(\"model/mymodel\")\n",
        "\n",
        "\n",
        "train_dataset = tf.data.Dataset.from_generator(create_train_generator, output_types=(tf.float32, tf.float32), output_shapes=((5, 1000, 501), (5, 1000, 501)))\n",
        "validation_dataset = tf.data.Dataset.from_generator(create_val_generator, output_types=(tf.float32, tf.float32), output_shapes=((5, 1000, 501), (5, 1000, 501)))\n",
        "\n",
        "model.fit(train_dataset, epochs=50, shuffle=True, callbacks=[tensorboard_callback],\\\n",
        "          steps_per_epoch=1000, validation_data=validation_dataset, validation_steps=250, verbose=1)\n",
        "\n",
        "model.save('./model/mymodel')\n",
        "print('model saved')\n"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "iqSDQF0AOmKr"
      },
      "source": [
        "model.save('./model/mymodel')\n",
        "print('model saved')"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "dDP50CX6fbM_"
      },
      "source": [
        "## **VISUALISATION**\n"
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "THV1P5I678Qy"
      },
      "source": [
        "# !kill 8599"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "EZYuuj9x5Y_s"
      },
      "source": [
        "%tensorboard --logdir logs/fit --host localhost"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "6so_2SaUGmHt"
      },
      "source": [
        " ## **AUDIO RECONVERSION**"
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "xcgQig1r1CnL"
      },
      "source": [
        "def load_and_convert_data(train_mp3_data, add_noise_to_sample=False):\n",
        "    \"\"\"\n",
        "    function to load mp3 data, only the sample, and convert it to array, then to f-t-domain\n",
        "    -- train_mp3_data : list of mp3 data to load\n",
        "    -- add_noise_to_sample : True/False indicate if noise has to be added\n",
        "    -- return : abs and im part of the sample, some characteristic of the sample\n",
        "    \"\"\"\n",
        "    samples = []\n",
        "    im_samples = []\n",
        "    i = 0\n",
        "\n",
        "    for path_to_mp3_file in tqdm(train_mp3_data):\n",
        "\n",
        "        # load mp3 sound to array\n",
        "        loaded_sound = AudioSegment.from_mp3(path_to_mp3_file)\n",
        "        array_sound = np.array(loaded_sound.get_array_of_samples(), np.float64)\n",
        "\n",
        "        len_arr = len(array_sound)\n",
        "\n",
        "        plt.plot([i for i in range(len(array_sound))], array_sound, 'b')\n",
        "        plt.show()\n",
        "\n",
        "        # normalize array\n",
        "        mean = np.mean(array_sound, axis=0)\n",
        "        array_sound -= mean\n",
        "\n",
        "        if add_noise_to_sample:\n",
        "            array_sound = add_noise(array_sound)\n",
        "            # save reconverted sound to wav\n",
        "            path_to_wav_file = './sons/noisy/noisy_sound0.wav'\n",
        "            write(path_to_wav_file, 48000, array_sound)\n",
        "\n",
        "        max_arr = np.max(np.abs(array_sound))\n",
        "        array_sound = np.array(array_sound/max_arr, np.float32)\n",
        "\n",
        "        # convert sample to to F_T_space\n",
        "        f, t, array_stft = stft(array_sound, fs=48000, nperseg=1000)\n",
        "        sample_abs_stft = np.abs(array_stft)\n",
        "        sample_im_stft = np.imag(array_stft)\n",
        "\n",
        "        # padding \n",
        "        pad_length = 1000 - np.shape(sample_abs_stft)[1]\n",
        "        sample_abs_stft = np.pad(sample_abs_stft, ((0,0), (0, pad_length)))\n",
        "        sample_im_stft = np.pad(sample_im_stft, ((0,0), (0, pad_length)))\n",
        "\n",
        "        samples.append(sample_abs_stft)\n",
        "        im_samples.append(sample_im_stft)\n",
        "        i+=1\n",
        "\n",
        "    samples = np.array(samples)\n",
        "    im_samples = np.array(im_samples)\n",
        "\n",
        "    return samples, im_samples, mean, max_arr, len_arr"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "SdUwkCvMo_PQ"
      },
      "source": [
        "def denoising(samples, imag_samples, mean, max_arr, len_arr):\n",
        "    \"\"\"\n",
        "    function to denoise data by loading the model, predict and convert back to array of sound\n",
        "    -- samples : sample to denoise\n",
        "    -- imag_samples : im part of the sample to denoise\n",
        "    -- some characteristics of the sample\n",
        "    -- return : the denoised array\n",
        "    \"\"\"\n",
        "    # load model\n",
        "    model = tf.keras.models.load_model('./model/mymodel')\n",
        "\n",
        "    # reshape data\n",
        "    samples = np.squeeze(samples)\n",
        "    imag_samples = np.squeeze(imag_samples)\n",
        "    samples = np.transpose(samples)\n",
        "    imag_samples = np.transpose(imag_samples)\n",
        "    samples = np.expand_dims(samples, axis=0)\n",
        "    imag_samples = np.expand_dims(imag_samples, axis=0)\n",
        "    # print(np.shape(samples))\n",
        "\n",
        "    # predict\n",
        "    denoised_samples = model.predict(samples)\n",
        "\n",
        "    denoised_samples = np.transpose(denoised_samples)\n",
        "    imag_samples = np.transpose(imag_samples)\n",
        "\n",
        "    # print(np.shape(denoised_samples))\n",
        "\n",
        "    # plt.pcolormesh(denoised_samples[:len_arr,:])\n",
        "    # plt.title('STFT denoised Magnitude')\n",
        "    # plt.ylabel('Frequency [Hz]')\n",
        "    # plt.xlabel('Time [sec]')\n",
        "    # plt.show()\n",
        "\n",
        "    denoised_stft = denoised_samples + 1j*imag_samples\n",
        "    denoised_stft = np.squeeze(denoised_stft)\n",
        "    print(np.shape(denoised_stft))\n",
        "\n",
        "    # convert back to sound from F_T_space\n",
        "    t, desnoised_array_sound = istft(denoised_stft, fs=48000, nperseg=1000)\n",
        "    desnoised_array_sound = np.array(desnoised_array_sound)\n",
        "    desnoised_array_sound *= max_arr\n",
        "    desnoised_array_sound += mean\n",
        "    desnoised_array_sound = np.array(desnoised_array_sound, np.int16)\n",
        "\n",
        "    return desnoised_array_sound\n"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "zHbpPOU0yjet"
      },
      "source": [
        "# load a mp3 file, convert it to array and then to f-t-domain, add noise, predict the denoised array and save it\n",
        "\n",
        "list_files = [\"./sons/original/common_voice_fr_18157595.mp3\"]\n",
        "samples, imag_samples, mean, max_arr, len_arr = load_and_convert_data(list_files, True)\n",
        "\n",
        "denoised_array_sound = denoising(samples, imag_samples, mean, max_arr, len_arr)\n",
        "# print(len_arr)\n",
        "denoised_array_sound = denoised_array_sound[:len_arr]\n",
        "print(np.shape(denoised_array_sound))\n",
        "\n",
        "plt.plot([i/48000 for i in range(len_arr)], denoised_array_sound, 'r')\n",
        "plt.show()\n",
        "\n",
        "# print(np.shape(denoi sed_samples))\n",
        "samples, x = reshape_data(samples)\n",
        "samples = np.transpose(samples)\n",
        "samples = samples[:len_arr,:]\n",
        "\n",
        "# print(np.shape(samples))\n",
        "# print(np.shape(denoised_array_sound))\n",
        "\n",
        "# plt.pcolormesh(samples)\n",
        "# plt.title('STFT sample Magnitude')\n",
        "# plt.ylabel('Frequency [Hz]')\n",
        "# plt.xlabel('Time [sec]')\n",
        "# plt.show()\n",
        "\n",
        "# save reconverted sound to wav\n",
        "path_to_wav_file = './sons/denoised/denoised_sound0.wav'\n",
        "write(path_to_wav_file, 48000, denoised_array_sound)\n",
        "    "
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "1r_ZimvY7r0j"
      },
      "source": [
        "!ls model/mymodel/assets"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "5rrtLuCRfpHW"
      },
      "source": [
        "## **TEST**"
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "3BOZbbKGg64v",
        "outputId": "767bdfa5-1d44-4738-ade8-a1b67cf754a2",
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 358
        }
      },
      "source": [
        "# load mp3 sound to array\n",
        "mp3_sound = AudioSegment.from_mp3(\"./sons/original/common_voice_fr_18157595.mp3\")\n",
        "array_sound_mp3 = np.array(mp3_sound.get_array_of_samples())\n",
        "\n",
        "# save original to wav\n",
        "path_to_wav_file = './sons/noised/noised_sound0.wav'\n",
        "write(path_to_wav_file, 48000, array_sound_mp3)\n",
        "\n",
        "# save to mp3 from wav\n",
        "mp3_sound_save = AudioSegment.from_wav(path_to_wav_file)\n",
        "mp3_sound_save.export('./sons/original/noised_sound0.mp3', format='mp3')\n",
        "\n",
        "# load wav sound to array\n",
        "samplerate, wav_sound = read(\"./sons/noised/noised_sound0.wav\")\n",
        "array_sound_wav = np.array(wav_sound, np.float64)\n",
        "\n",
        "# # plot\n",
        "# plt.plot([i for i  in range(len(array_sound_wav))], array_sound_wav, 'b')\n",
        "# plt.show()\n",
        "# plt.plot([i for i  in range(len(array_sound_mp3))], array_sound_mp3, 'r')\n",
        "# plt.show()\n"
      ],
      "execution_count": null,
      "outputs": [
        {
          "output_type": "error",
          "ename": "FileNotFoundError",
          "evalue": "ignored",
          "traceback": [
            "\u001b[0;31m---------------------------------------------------------------------------\u001b[0m",
            "\u001b[0;31mFileNotFoundError\u001b[0m                         Traceback (most recent call last)",
            "\u001b[0;32m<ipython-input-106-118c0885b7aa>\u001b[0m in \u001b[0;36m<module>\u001b[0;34m()\u001b[0m\n\u001b[1;32m      5\u001b[0m \u001b[0;31m# save original to wav\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m      6\u001b[0m \u001b[0mpath_to_wav_file\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0;34m'./sons/noised/noised_sound0.wav'\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m----> 7\u001b[0;31m \u001b[0mwrite\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mpath_to_wav_file\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0;36m48000\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0marray_sound_mp3\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m      8\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m      9\u001b[0m \u001b[0;31m# save to mp3 from wav\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
            "\u001b[0;32m/usr/local/lib/python3.6/dist-packages/scipy/io/wavfile.py\u001b[0m in \u001b[0;36mwrite\u001b[0;34m(filename, rate, data)\u001b[0m\n\u001b[1;32m    380\u001b[0m         \u001b[0mfid\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mfilename\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    381\u001b[0m     \u001b[0;32melse\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m--> 382\u001b[0;31m         \u001b[0mfid\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mopen\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mfilename\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0;34m'wb'\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m    383\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    384\u001b[0m     \u001b[0mfs\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mrate\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
            "\u001b[0;31mFileNotFoundError\u001b[0m: [Errno 2] No such file or directory: './sons/noised/noised_sound0.wav'"
          ]
        }
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "8oe1HBz36itI",
        "outputId": "19f68bc2-8697-4526-e769-464fd11b8010",
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 276
        }
      },
      "source": [
        "for i, path_to_mp3_file in enumerate(list_mp3_files[1:2]):\n",
        "\n",
        "    # load mp3 sound to array\n",
        "    loaded_sound = AudioSegment.from_mp3(path_to_mp3_file)\n",
        "    array_sound = np.array(loaded_sound.get_array_of_samples(), np.float64)\n",
        "\n",
        "    # normalize array\n",
        "    mean = np.mean(array_sound, axis=0)\n",
        "    std = np.std(array_sound)\n",
        "    array_sound -= mean\n",
        "    array_sound /= std\n",
        "\n",
        "    # add random noise\n",
        "    max_1 = max(array_sound)\n",
        "    noise = np.random.normal(0, 0.75, len(array_sound))\n",
        "    array_sound_noisy = np.add(noise, array_sound)\n",
        "    max_2 = max(array_sound_noisy)\n",
        "    array_sound_noisy *= max_1/max_2\n",
        "    \n",
        "    # # save original to wav\n",
        "    # array_sound_noisy_to_save = array_sound_noisy * std\n",
        "    # array_sound_noisy_to_save += mean\n",
        "    # array_sound_noisy_to_save = np.array(array_sound_noisy_to_save, np.int16)\n",
        "    # path_to_wav_file = './noised/noised_sound0.wav'\n",
        "    # write(path_to_wav_file, 48000, array_sound_noisy_to_save)\n",
        "\n",
        "    # denoised = np.where(np.sqrt(array_sound_noisy**2-array_sound**2)>array_sound_noisy/3, 0, array_sound_noisy)\n",
        "    # mask = np.where(0<=(array_sound**2)/(noise**2),(array_sound**2)/(noise**2), 0)\n",
        "    # mask = np.where(mask<1,mask, 1)\n",
        "    mask = (array_sound**2)/(noise**2)\n",
        "\n",
        "    denoised = array_sound_noisy * mask\n",
        "\n",
        "    # plot original and noisy\n",
        "    # plt.plot([i for i  in range(len(noise))], noise, 'r')\n",
        "    # plt.plot([i for i  in range(len(array_sound))], array_sound, 'b')\n",
        "    # plt.plot([50000+i for i  in range(1000)], mask[50000:51000], 'r')\n",
        "    plt.plot([i for i  in range(len(denoised))], denoised, 'r')\n",
        "    plt.show()\n",
        "\n",
        "    # save reconverted sound to wav\n",
        "    path_to_wav_file = './denoised/denoised_sound0.wav'\n",
        "    write(path_to_wav_file, 48000, denoised)\n",
        "\n",
        "    # # convert sound to to F_T_space\n",
        "    # f, t, array_stft = stft(array_sound_noisy, fs=48000, nperseg=1000)\n",
        "    # array_abs_stft = np.abs(array_stft)\n",
        "\n",
        "    # plt.pcolormesh(t, f, array_abs_stft)\n",
        "    # plt.title('STFT Magnitude')\n",
        "    # plt.ylabel('Frequency [Hz]')\n",
        "    # plt.xlabel('Time [sec]')\n",
        "    # plt.show()\n",
        "\n",
        "    # # convert back to sound from F_T_space\n",
        "    # t, desnoised_array_sound = istft(array_stft, fs=48000, nperseg=1000)\n",
        "    # desnoised_array_sound = np.array(desnoised_array_sound)\n",
        "    # desnoised_array_sound *= std\n",
        "    # desnoised_array_sound += mean\n",
        "    # desnoised_array_sound = np.array(desnoised_array_sound, np.int16)\n",
        "\n",
        "    # # save reconverted sound to wav\n",
        "    # path_to_wav_file = './denoised/denoised_sound' + str(i) + '.wav'\n",
        "    # write(path_to_wav_file, 48000, desnoised_array_sound)\n",
        "    "
      ],
      "execution_count": null,
      "outputs": [
        {
          "output_type": "display_data",
          "data": {
            "image/png": "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\n",
            "text/plain": [
              "<Figure size 432x288 with 1 Axes>"
            ]
          },
          "metadata": {
            "tags": [],
            "needs_background": "light"
          }
        }
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "phNo-Z72foqY",
        "outputId": "25d5309e-edcd-4021-e142-f541d1bdfcc5",
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 69
        }
      },
      "source": [
        "for path_to_mp3_file in tqdm(train_mp3_data):\n",
        "    # print(i)\n",
        "    a =1"
      ],
      "execution_count": null,
      "outputs": [
        {
          "output_type": "stream",
          "text": [
            "\n",
            "\n",
            "100%|██████████| 500/500 [00:00<00:00, 310643.16it/s]\n"
          ],
          "name": "stderr"
        }
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "zh0r2hNIDaCT",
        "outputId": "29bad297-45cb-4617-c6c3-04b3ccbccf83",
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 136,
          "referenced_widgets": [
            "94e6fdeca98a4d0b8c7ce52e6162ecd0",
            "774c0af238a24314b0257646698fe490",
            "96a124720fdb42a3a7e025a3329d440a",
            "4df34abfa3eb41c5981bbd460670739a",
            "1918b54b8e624da485e46afc32cff7f2",
            "37ae2d8f98424c71951abecbdb32617e",
            "627ed32a482a4a5dac594dcb0e61186e",
            "cb367e16a13a4bda87e91a76d8100782"
          ]
        }
      },
      "source": [
        "\n",
        "samples, labels, mean, std = load_and_convert_data_with_label(list_mp3_files[1:2])\n",
        "\n",
        "print(np.shape(samples))\n",
        "print(np.shape(labels))\n",
        "\n",
        "desnoised_array_stft = samples[0]*labels[0]\n",
        "\n",
        "# convert back to sound from F_T_space\n",
        "t, desnoised_array_sound = istft(desnoised_array_stft, fs=48000, nperseg=1000)\n",
        "desnoised_array_sound = np.array(desnoised_array_sound)\n",
        "desnoised_array_sound *= std\n",
        "desnoised_array_sound += mean\n",
        "desnoised_array_sound = np.array(desnoised_array_sound, np.int16)\n",
        "\n",
        "# save reconverted sound to wav\n",
        "path_to_wav_file = './denoised/denoised_sound0.wav'\n",
        "write(path_to_wav_file, 48000, desnoised_array_sound)\n",
        "\n"
      ],
      "execution_count": null,
      "outputs": [
        {
          "output_type": "display_data",
          "data": {
            "application/vnd.jupyter.widget-view+json": {
              "model_id": "94e6fdeca98a4d0b8c7ce52e6162ecd0",
              "version_minor": 0,
              "version_major": 2
            },
            "text/plain": [
              "HBox(children=(FloatProgress(value=0.0, max=1.0), HTML(value='')))"
            ]
          },
          "metadata": {
            "tags": []
          }
        },
        {
          "output_type": "stream",
          "text": [
            "\n",
            "(1, 501, 2000)\n",
            "(1, 501, 2000)\n"
          ],
          "name": "stdout"
        },
        {
          "output_type": "stream",
          "text": [
            "/usr/local/lib/python3.6/dist-packages/ipykernel_launcher.py:38: RuntimeWarning: invalid value encountered in sqrt\n",
            "/usr/local/lib/python3.6/dist-packages/ipykernel_launcher.py:38: RuntimeWarning: invalid value encountered in greater\n"
          ],
          "name": "stderr"
        }
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "pSQfBJPPGy9K",
        "outputId": "0a36323b-00f9-4388-983f-c186a18fae00",
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 34
        }
      },
      "source": [
        "a = np.array([ 1 + 1j, 1 + 1j, 1 + 1j])\n",
        "b = np.abs(a)\n",
        "c = np.imag(a)\n",
        "d = b + 1j*c\n",
        "print(d)"
      ],
      "execution_count": null,
      "outputs": [
        {
          "output_type": "stream",
          "text": [
            "[1.41421356+1.j 1.41421356+1.j 1.41421356+1.j]\n"
          ],
          "name": "stdout"
        }
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "jc1dlb0R7h8t"
      },
      "source": [
        "def num_conversion(x):\n",
        "    x = np.uint16(x)\n",
        "    result = []\n",
        "    if x < 0:\n",
        "        x = 0\n",
        "    if x >= 65024:\n",
        "        x = 65024\n",
        "    if 0 <= x <= 65024:\n",
        "        x_0 = x - x % 1000\n",
        "        x_1 = x - x_0 - x % 10\n",
        "        result = [x_0//1000, x_1//10]\n",
        "    else:\n",
        "        print(\"error, wrong input\")\n",
        "        print(x)\n",
        "    return result\n",
        "\n",
        "def im_conversion(im):\n",
        "    im = np.array(im, np.int16)\n",
        "    s = np.shape(im)\n",
        "    s = [s[0], s[1], 2]\n",
        "    result = np.zeros(s)\n",
        "    for i in range(s[0]):\n",
        "        for j in range(s[1]):\n",
        "            result[i, j] = num_conversion(im[i, j])\n",
        "    return np.uint16(result)"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "_x4tjzGFN28u"
      },
      "source": [
        "def num_conversion_back(x_0, x_1):\n",
        "    number = np.uint16(x_0*1000 + x_1*10) \n",
        "    return number\n",
        "    \n",
        "def im_conversion_back(im_0, im_1):\n",
        "    im_0 = np.array(im_0, np.int16)\n",
        "    im_1 = np.array(im_1, np.int16)\n",
        "    s = np.shape(im_0)\n",
        "    s = [s[0], s[1]]\n",
        "    result = np.zeros(s)\n",
        "    for i in range(s[0]):\n",
        "        for j in range(s[1]):\n",
        "            result[i, j] = num_conversion_back(im_0[i, j], im_1[i, j])\n",
        "    return np.uint16(result)"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "P9KkuU5XO6yR"
      },
      "source": [
        "## **TEST2**\n"
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "uaguayXtO9Ue"
      },
      "source": [
        "def build_model():\n",
        "    \"\"\"\n",
        "    function to build the model\n",
        "    -- return the compiled model\n",
        "    \"\"\"\n",
        "    model = models.Sequential()\n",
        "    model.add(layers.SeparableConv2D(16, (3, 3), activation='relu', padding='same', input_shape=(1000, 501, 1)))\n",
        "    model.add(layers.SeparableConv2D(32, (3, 3), activation='relu', padding='same'))\n",
        "    model.add(layers.SeparableConv2D(1, (3, 3), activation='relu', padding='same'))\n",
        "\n",
        "    model.compile(optimizer='rmsprop', loss='mse', metrics=['mae'])\n",
        "\n",
        "    return model"
      ],
      "execution_count": 23,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "ZqXxYSbDPCTz",
        "outputId": "345d79d7-a238-439e-cca7-ac7e27419686",
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 89
        }
      },
      "source": [
        "model = build_model()\n",
        "\n",
        "arr = np.zeros((1000, 501, 1))\n",
        "arr = np.expand_dims(arr, axis=0)\n",
        "print('arr shape', np.shape(arr))\n",
        "\n",
        "pred = model.predict(arr)\n",
        "print('pred shape', np.shape(pred))"
      ],
      "execution_count": 24,
      "outputs": [
        {
          "output_type": "stream",
          "text": [
            "arr shape (1, 1000, 501, 1)\n",
            "WARNING:tensorflow:11 out of the last 11 calls to <function Model.make_predict_function.<locals>.predict_function at 0x7ffb82720ea0> triggered tf.function retracing. Tracing is expensive and the excessive number of tracings could be due to (1) creating @tf.function repeatedly in a loop, (2) passing tensors with different shapes, (3) passing Python objects instead of tensors. For (1), please define your @tf.function outside of the loop. For (2), @tf.function has experimental_relax_shapes=True option that relaxes argument shapes that can avoid unnecessary retracing. For (3), please refer to https://www.tensorflow.org/tutorials/customization/performance#python_or_tensor_args and https://www.tensorflow.org/api_docs/python/tf/function for  more details.\n",
            "pred shape (1, 1000, 501, 1)\n"
          ],
          "name": "stdout"
        }
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "R2m-EFU8RMWw",
        "outputId": "6b438cc0-629b-4d76-fa7d-408635d506f6",
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 34
        }
      },
      "source": [
        "x = range(-5, 5) \n",
        "all_less_than_zero = list(filter(lambda num: num < 0, x))\n",
        "all_less_than_zero"
      ],
      "execution_count": 26,
      "outputs": [
        {
          "output_type": "execute_result",
          "data": {
            "text/plain": [
              "[-5, -4, -3, -2, -1]"
            ]
          },
          "metadata": {
            "tags": []
          },
          "execution_count": 26
        }
      ]
    }
  ]
}